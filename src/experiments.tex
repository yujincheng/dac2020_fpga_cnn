\label{sec:experiments}




% In this section, the evaluation of the instruction-based-interruption, hardware modules for FE post-processing, and the overall MR-Exploration system are presented and analyzed.



\subsection{ Experiment Setup }

The hardware-in-loop evaluate environment is illustrated in \Cref{fig:env}(a). There is a simulation server providing the simulation environment based on AirSim \cite{shah2018airsim}. The AirSim simulator provides the camera data for the two agents. Two Xilinx ZCU102 boards \cite{zcu102}, with ZU9 MPSoC \cite{MPSoC}, are responsible for the calculation of each agent.
SuperPoint \cite{detone2018superpoint} is used for extracting feature-points. GeM \cite{radenovic2018fine} is used to implement the PR module, which is based on ResNet101 \cite{he2016deep}. 

The CNN is calculated by the Angel-Eye CNN accelerator \cite{guo2017angel} on the FPGA side of ZU9 MPSoC, and other operations are on the CPU side.
For a precise evaluation the CNN running time, we record the clock cycles of the beginning and end of each instruction. The time of the interrupt response latency and the total cost in the following evaluation is calculated from the clock cycles and the clock frequence.

% The hardware resources are listed in \Cref{tab:hardware}. The hardware resources are provided by Vivado after hardware implementation. Vivado \cite{Vivado} is the development toolchain for MPSoC provided by Xilinx. The CNN backbone is calculated by the Angel-Eye CNN accelerator \cite{guo2017angel} on the FPGA side of ZCU102 (Programmable logic, PL side). The FE post-processing steps run on our proposed accelerators, also on the PL side. The PL side has 2 clock frequencies. The CNN accelerator and the IAU are running at 300MHz. The accelerator for FE post-processing is running at 200MHz. Compared with the CNN accelerator, IAU and FE post-processing use very little hardware resources.

% % Table generated by Excel2LaTeX from sheet 'Sheet1'
% \begin{table}[t]
% \centering
% % \setlength{\abovecaptionskip}{2pt}
% \caption{Hardware consumption of the proposed hardware}
% % Table generated by Excel2LaTeX from sheet 'Sheet1'
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% & $\# DSP$ & $\# LUT$ & $\# FF$ & $\# BRAM$ \\
% \hline
% On-Board resource & 2520 & 274080 & 548160 & 912 \\
% \hline
% CNN accelerator & 1282 & 74569 & 171416 & 499 \\
% \hline
% IAU & 0 & 2268 & 4633 & 4 \\
% \hline
% FE post-processing & 25 & 17573 & 29115 & 10 \\
% \hline
% \end{tabular}%
 
% \label{tab:hardware}%
% \end{table}%

% \begin{figure}[t]
% \centering
% % \vspace{-0.1cm} 
% % \setlength{\abovecaptionskip}{0cm} 
% % \setlength{\belowcaptionskip}{-0.05cm} 
% \includegraphics[width=0.99\linewidth]{fig/PRresult.pdf}
% \caption{The interrupt response latency \& extra time cost.}
% \label{fig:scatter1024}
% \end{figure}


\subsection{Virtual Instruction-based interrupt }

In MR-Exploration, only the low-priority PR task is interruptible, and the interrupt position is unpredictable. GeM \cite{radenovic2018fine} is used to implement the PR module in the experiment.
The CNN backbone of the GeM is ResNet101 \cite{he2016deep}, which contains 101 convolution layers. The input shape of the CNN is $480 \times 640 \times 3$. The parallelism of the Angel-Eye is $Para_{height}=8$, $Para_{in}=16$, $Para_{out}=16$. 
We randomly sample 12 positions of the ResNet101 CNN backbone. The interrupt response latency and the extra time cost for different implementation of interrupt at the positions are listed in \Cref{fig:barresult}(a).
The CPU-like interrupt consumes the most extra cost ($t_{cost}$). Though the layer-by-layer interrupt consumes no extra time, the latency is much higher than our virtual-instruction-based interrupt. 
This is because the layer-by-layer interrupt need to wait for completion of a layer. The performance at same interrupt position in our proposed virtual interrupt can interrupt inside a layer, with lower latency.

\label{sec:viexp}

Furthermore, though the network structures differ between different CNNs, the convolutional layers, which are the basic component in CNN, are similar between different CNNs. INCA monitors the running status inside each layer, and the interrupt respond latency and extra cost are only relevant to the currently operating layer. 
We compare the interrupt respond latency of our VI method with layer-by-layer method at different layers from different networks, including ResNet \cite{he2016deep}, VGG \cite{simonyan2014very}, and MobileNet \cite{howard2017mobilenets}. The results is illustrated in \Cref{fig:barresult}(b). We evaluate our method on both th big accelerator with large hardware parallelism and small accelerator with small parallelism. 
In the ResNet and VGG, the average interrupt respond latency of the layer-by-layer method is $ms$ to tens of $ms$, which makes the high priority task with hard deadline in embedded system unable to be completed on time. With our VI method, the latency can be reduced to less than 100$us$, so that the high priority task can be started immediately and completed on time. 
For the lightweight network (MobileNet), although the latency of the layer-by-layer method is about 1ms, we can still reduce the latency by 2-3 orders of magnitude with VI method. This result also matches the theoretical analysis in \Cref{equ:rate}.



% The latency to respond the interrupt in CPU-like method consists of the time to finish current executing instruction and the data backup time ($t_{latency} = t_1+t_2$) for the on-chip data/weights caches (totally 2.2MB). The latency in layer-by-layer interrupt is the time to finish current layer. The latency of our virtual-instruction-based method is the time to finish current executing instruction and the backup time for the calculated output results. 
% The cost of CPU-like interrupt is the data transfer time of all the on-chip caches (totally 2.2MB) to/from DDR ($t_{cost} = t_2+t_4$). The cost of virtual-instruction-based method is only the recovery of the input/weights from DDR to on-chip caches ($t_{cost} = t_4$). There is no extra cost for the layer-by-layer interrupt.

% 


% \subsubsection{ Time comparison between $t_1$ and $t_2$ }

% As described in \Cref{sec:virtualinstr}, the Layer-By-Layer interrupt method do not need to backup data before interruption ($t_2 = 0$). Though our Virtual-Instruction method (VI method) need to spend time to backup the final results, which are already generated yet not stored to DDR ($t_2$). However, compared with computation, the time of data backup ($t_2$) is short. We list the backup time and the convolution time ($t_1$) at some of the interrupt position in \Cref{fig:scatter1024}(b), with different featuremap shape, kernel size, and input/output channels. $H$, $W$ are the height, width of input featuremaps. $Ch_{in}$, $Ch_{out}$ are the number of input and output channels. The time of backup and calculation is listed in $Backup$ and $Conv$ columns. The ratio of backup and calculation are listed in the last column. The backup operation only consumes less than 20\% of the calculation time. For the first layer (first line of \Cref{tab:timecompare}). The input channels number is too small, so the calculation time is also short. Therefore, the backup time has reached half of the calculation time. 

% % Table generated by Excel2LaTeX from sheet 'Sheet4'
% \begin{table}[t]
% \centering
% % \footnotesize
% \caption{Time comparison between data backup and calculation}
% \begin{tabular}{|c|c|c|c|c|c|c|c|}
% \hline
% \multirow{2}[2]{*}{$H$} & \multirow{2}[2]{*}{$W$} & \multirow{2}[2]{*}{$Ch_{in}$} & \multirow{2}[2]{*}{$Ch_{out}$} & Kernel & Backup & Conv & \multirow{2}[2]{*}{$\frac{Backup}{Conv}$} \bigstrut[t]\\
% & & & & Size & ($t_2$,$us$) & ($t1$,$us$) & \\
% \hline
% 480 & 640 & 3 & 64 & $7 \times 7$ & 26.29 & 52.38 & 50.2\% \\
% \hline
% 120 & 160 & 128 & 128 & $3 \times 3$ & 8.77 & 41.18 & 21.3\% \\
% \hline
% 30 & 40 & 1024 & 2048 & $1 \times 1$ & 1.25 & 8.75 & 14.3\% \\
% \hline
% 30 & 40 & 512 & 512 & $3 \times 3$ & 1.42 & 39.36 & 3.6\% \\
% \hline
% 16 & 20 & 512 & 512 & $3 \times 3$ & 0.75 & 20.16 & 3.8\% \\
% \hline
% \end{tabular}%
% \label{tab:timecompare}%
% \end{table}%


\subsection{ ROS based MR-Exploration }


The results of the Multi-Robot Exploration based on INCA is shown in \Cref{fig:env}. The space in the AirSim \cite{shah2018airsim} for the robots to explore is shown in \Cref{fig:env}(a). It is a simple rectangle area with four different pillars, and some chairs at the center (in the white box). \Cref{fig:env}(b) shows how place recognition (PR) works for map merging. The visual odometry (VO) based on feature-points on each agent produce the local map and trajectory. When the PR threads find out a similar scene, and the maps and the trajectories are merged via the similar scene, as shown in \Cref{fig:env}(c).

In this example, the CNN-based feature-point extraction (FE) and place recognition (PR) are both executed on the same Angel-Eye \cite{guo2017angel} accelerator. The frequency of the input camera is 20fps, and each input frame is fed to the FE, and FE module would take up accelerator. While the CPU process VO with the feature-points from FE, the accelerator can switch to process the low-priority PR task, as illustrated in \Cref{fig:inca}(a). Thus, the PR process one frame every 7$\sim$10 input frames.
However, the scene between adjacent frames is similar, so it is not necessary to do place recognition for each input picture. Place recognition every 10 frames can meet the task requirements of DSLAM.