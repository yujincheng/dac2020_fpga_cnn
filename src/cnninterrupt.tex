\label{sec:cnninterrupt}


% Table generated by Excel2LaTeX from sheet 'Sheet3'
\begin{table}[t]
	\centering
	\scriptsize
	\caption{Description for the basic instructions}
% Table generated by Excel2LaTeX from sheet 'Sheet5'
% Table generated by Excel2LaTeX from sheet 'Sheet5'
\begin{tabular}{|p{4em}|p{12em}||p{6em}|p{6em}|}
	\hline
	Type & Description & Backups & Recovery \bigstrut\\
	\hline
	LOAD\_W & Load weights/bias from DDR to on chip weight buffer. & - & Weight / Inputdata \bigstrut\\
	\hline
	LOAD\_D & Load input featuremaps from DDR to on chip weight buffer. & - & Weight / Inputdata \bigstrut\\
	\hline
	CALC\_I & Calculate intermediate results for some output channels from partial input channels. &  Intemediate data & Weight / Inputdata / intemediate data \bigstrut\\
	\hline
	CALC\_F & Calculate the results for some output channels from all input channels. & Finial results & Weight / Inputdata \bigstrut\\
	\hline
	SAVE & Save the results from on-chip data buffer to DDR. & - & Weight / Inputdata \bigstrut\\
	\hline
	\end{tabular}%
	
	\label{tab:instr}%
 \end{table}%
 

% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=0.9\linewidth]{fig/instructions.png}
% 	\caption{Original and Virtual-Instruction ISA.}
% 	\label{fig:instructions}
% \end{figure}



\subsection{ Instruction Driven Accelerator }
\label{sec:instrAcc}
There are three categories of instruction in the instruction-driven accelerator: LOAD (LOAD\_W / LOAD\_D), CALC (CALC\_I / CALC\_F), and SAVE \cite{guo2017angel,qiu2016going,yu2018instruction}. The instruction description of each kind of instruction is listed in \Cref{tab:instr}. 

Each CALC instruction, including CALC\_I and CALC\_F, processes the convolution according to the hardware parallelism with $Para_{height}$ lines from $ Para_{in} $ input channels to $ Para_{out}$ output channels. $Para_{height}$, $ Para_{in} $, and $ Para_{out} $ are the parallelism along the height, input channel and output channel dimensions, which is determined by the hardware and original ISA. The convolution of the last $ Para_{in} $ input channels is CALC\_F, and the convolutions for the former input channels are CALC\_I, as illustrated in \Cref{fig:singlesave}(a). The CALC\_F and the CALC\_I instructions for the same output channels, as well as the LOAD instructions for corresponding input feature-maps and weights, are considered as a \textbf{CalcBlob}.


\begin{figure}[t]
 \centering
	\includegraphics[width=0.99\linewidth]{fig/singlesave.pdf} 	
 \caption{
		Scheduling and interrupt for the instruction-driven accelerator.
 }
	\label{fig:singlesave}
\end{figure}

\subsection{How To Interrupt: Virtual Instruction}
\label{sec:howinter}

There are four stages to handle interrupt. For the instruction flow illustrated in \Cref{fig:singlesave}(b), the interrupt stages are shown in \Cref{fig:singlesave}(c), including: (1) Time for finishing the current operation, $t1$. (2) Time to backup, $t2$. (3) Time for the high-priority task, $t3$. (4) Time to restore the low-priority task ,$t4$. The the latency to respond the interrupt is $t_{latency} = t_1+t_2$. The extra cost for interrupt is $t_{cost}=t_2+t_4$. 
There are different methods to implement interrupt in CNN accelerators.

\textbf{CPU-Like.}
When an interrupt request occurs in CPU, CPU backs up all the on-chip registers to DDR. However, there are only tens of registers in CPU, and the volume of the backed-up data is less than 1 KB \cite{furber2000arm}. In CNN accelerators, there are several $MB$ of on-chip caches \cite{qiu2016going, guo2017angel} for input feature-maps and weights. 
% If all on-chip caches are backed-up/recovered, the cost of data transfer in the accelerator is much higher than that of CPU. 
Thus, the extra data transfer increases both the interrupt response latency($t_{latency}$) and the additional cost ($t_{cost}$).

\textbf{Layer-by-layer.}
Most accelerators run the CNN layer by layer \cite{qiu2016going,guo2017angel}. 
There is no extra data transfer for the accelerator to switch between different tasks after each layer, thus, $t_{cost}=0$. 
However, the position of the interrupt request is irregular and unpredictable. When an interrupt occurs inside a CNN layer, the CNN accelerator needs to finish the whole layer before switching, which leads to the high response latency($t_{latency}$).

% The latency to respond the interrupt and the performance degradation of the CPU-like interrupt and Layer-by-layer method will be evaluated in \Cref{sec:experiments}.

We propose the \textbf{virtual-instruction-based} method (VI method) to enable low-latency interrupt. 
% Different from the CPU-like interrupt, which backup/recovery all the on-chip caches, only the on-chip cache which is still needed in future execution will be backed-up and restored. So that the amount of data transfer is much lower than that of CPU-like interrupt.
To reduce the interrupt response latency, our virtual-instruction-based method is interruptible inside each layer. 
Virtual instructions are some special instructions in the original instruction sequences.
If an interrupt occurs, virtual instructions are executed to back up and restore the running state.
If no interrupt occurs, the virtual instructions will not be executed.
We add some virtual instructions to the original instruction sequence to enable the interrupt, which contain the vitrual SAVE (Vir\_SAVE) and vitrual LOAD  (Vir\_LAOD) instructions, are responsible for backing up and restoring on-chip caches respectively. 

% \textbf{Virtual SAVE} instructions back up the intermediate results from partial input channels or the final output results. There is no need to back up the input feature-maps and weights, because these inputs are already stored in DDR. 

% \textbf{Virtual LOAD} instructions restore the input feature-maps from DDR to on-chip caches, because input featuremaps are loaded by one CalcBlob, and shared across subsequent CalcBlobs.
% , and thus the subsequent CalcBlobs do not read the input feature-maps. 
% Virtual LOAD instructions also need to restore the intermediate results from partial input channels backed up by the virtual SAVE instructions.


\subsection{ Where To Interrupt: After SAVE/CALC\_F }
\label{sec:whereinter}
% The virtual-instruction-based method has two potential factors that may lead to system performance degradation: 1) The extra data transfer to backup/restore running status takes up additional bandwidth resources. 2) The instruction fetching for the virtual instructions also uses bandwidth resources.
% Even they are skipped and discarded.
% To address the above problems of virtual-instruction-based method, 
We analyze the interrupt cost and select the positions of adding the virtual instructions.
The backup/recovery data for different interrupt positions at each kind of instruction are listed in the Backup/Recovery columns of \Cref{tab:instr}.

When an interruption occurs at LOAD, the newly loaded data are immediately flushed when running the high-level CNN, leading to bandwidth waste.

Compared with CALC\_I, when an interrupt occurs at CALC\_F, there are no intermediate results. 
Although it is necessary to back up the unsaved final results which are generated by previous CALC\_F, these results will be stored in DDR through the subsequent original SAVE instruction.
If the accelerator can record the interrupt status, we can modify the address and workload when executing subsequent original not-virtual SAVE instruction.
Thus, we can avoid the repetitive transmission of the final output results. The extra data transfer is only to recovery input data without any extra backup data, $t_{cost} = t_4$.

There is no data need to be backed up when interrupt after SAVE. The overhead of interrupt after SAVE is also only to restore input data from DDR to the on-chip caches, $t_{cost} = t_4$.

In order to minimize the cost of interrupt, we make the CNN interruptible after the SAVE or CALC\_F. This method only introduces extra data transfer to recovery input data without any extra backup data. Thus, $t_{cost} = t_4$, in our virtual-instruction-based interrupt.

% \begin{figure*}[t]
% 	\centering
% 	\subfloat[ $t_1$ for Layer-By-Layer method. ]{ \label{fig:t1all}
% 		\begin{minipage}[t]{0.45\linewidth}
% 			\centering
% 	\includegraphics[width=0.99\linewidth]{fig/t1all.pdf}
% 		\end{minipage}%
% 	}
% 	\subfloat[ $t_1$ for Virtual-Instruction method. ]{ \label{fig:t1after}
% 		\begin{minipage}[t]{0.45\linewidth}
% 			\centering
% 	\includegraphics[width=0.99\linewidth]{fig/t1after.pdf}
% 		\end{minipage}%
% 	}
% 	\label{fig:t1example}
% 	\caption{ Waiting time for finishing the current operation ($t_1$) in an example convolution layer. Compared with the Layer-By-Layer method, the waiting time of our Virtual-Instruction method is reduced to $1.6\%$ in this example. The reduction in latency is related to the height ($H$) of the input featuremaps. }
% \end{figure*}

Compared with Layer-by-layer interrupt, our method, which is interruptible after CALC\_F and SAVE, significantly reduces $t_{latency}$.
In the worst case, the interrupt request occurs at the beginning of the layer. In this case, the accelerator will wait until finishing the whole layer. The wait time is $t_{1\_layer}$:

\begin{equation*}
	t_{1\_layer} = \frac{ Ch_{in} \times Ch_{out} \times H }{ Para_{in} \times Para_{out} \times Para_{height} } \times t_{instr}(W)
\end{equation*}

Where $t_{instr}(W)$ is calculation time of a single CALC. The $W$ of the input featuremaps is larger, the time of a single CALC is longer.

The worst wait of our VI method is $t_{1\_VI}$:

\begin{equation*}
	t_{1\_VI} = \frac{ Ch_{in} \times Para_{out} \times Para_{height} }{ Para_{in} \times Para_{out} \times Para_{height} } \times t_{instr}(W)
\end{equation*}

Compared with the Layer-By-Layer method, the worst latency of our method is reduced to $R_l$.

\begin{equation}
	R_l = \frac{ t_{1\_VI} }{ t_{1\_layer} } = \frac{ Para_{out} \times Para_{height} }{ Ch_{out} \times H} 
	\label{equ:rate}
\end{equation}

The effect of latency reduction of the VI method is related to the number of output channels ($Ch_{out}$) and featuremap height ($H$). The larger the featuremaps output channels and the height, the better latency reduction result can be achieved.

For example, a medium-sized neural network layer, the input featuremap size is $80 \times 60$, the number of input channels is $CH_{in} = 48$, and the number of output channels is $CH_{out} = 32$. The instruction parallelism is restricted by the hardware architecture, whose input channel parallelism is $Para_{in} = 8$, output channel parallelism is $Para_{out} = 8$, height parallelism is $Para_{height} = 4$.
According to \Cref{equ:rate}, the latency is reduced to $\frac{ Para_{out} \times Para_{height} }{ Ch_{out} \times H} = 8 \times 4 / (32 \times 60) = 1.7\%$.

Since usually $Ch_{in} \gg Para_{out}$, compared with the time of finishing current calculation (need to read data from all $Ch_{in}$), the time of backing up the final result (only save data for a $Para_{out}$) can be ignored. So that in both VI and layer-bylayer methods, the interrupt respond latency $t_{latency}$ is mainly determined by waiting for current calculation $t_{1}$. Experimental results of VI method in \Cref{sec:viexp} include the backup time ($t_2$), yet the acceleration ratio is similar to the theoretical result of \Cref{equ:rate}.

% An example of a convolution layer with a typical size in CNN is given in \Cref{fig:t1example}. The parameters are labeled in the figures. The latency can be reduced to $1.6\%$.

\subsection{ Instruction Arrangement Unit (IAU) }

Instruction Arrangement Unit (IAU) is the hardware to handle the accelerator requirements from the tasks with different priorities. 
The hardware implementation of IAU is shown in \Cref{fig:IAU}, which supports 3 tasks with different priorities (priority 0,1,2).
Task 0 has the highest priority and is not interruptible.
Status Pool records the running status of the task at each priority. The Instruction Fetcher read the VI-ISA instruction sequences from DDR according to the running state (Run State) and the DDR address of instructions (Instr Addr). Virtual Instr FIFO decides whether a vitrual instructions needs to be executed according to the running state. 

SAVE Instruction Controller writes the status of the executed virtual instruction to Status Pool, including ID of the its corresponding normal SAVE (SaveID), the address and length of backed up final output (Save Addr and Save Length).
SAVE Instruction Controller also modifies the normal SAVE instruction according to the recorded ID, Addr, Length to avoid duplicate final output data transfer.

Instructions are also translated from VI-ISA to the original ISA via Instruction Translator and Ouput Instruction Controller, to be directly executed on the CNN accelerator. The CNN accelerator does not need to know the interrupt status, and only operates the instructions provided by IAU.

\begin{figure}[t]
	\centering
 % \vspace{-0.1cm} 
 % \setlength{\abovecaptionskip}{0cm} 
 % \setlength{\belowcaptionskip}{-0.4cm} 
	\includegraphics[width=0.9\linewidth]{fig/iau.pdf}
	\caption{Hardware architecture of IAU. 
	%Status Pool records the running status of the task at each priority. Other units complete address fetching, virtual instruction selection, instruction translation, according to the state recorded in Status Pool.
	% The software on the CPU (PS side) communicates with IAU to access the CNN accelerator. IAU records the running state of each task. At runtime, IAU translates the input instruction sequence with virtual instructions to a normal sequence of instructions. IAU also modifies the normal SAVE instruction after interrupt occurs with the same SaveID, to avoid duplicate output data transfer. 
	}
	\label{fig:IAU}
\end{figure}

\begin{figure}[t]
	\centering
 % \vspace{-0.1cm} 
 % \setlength{\abovecaptionskip}{0cm} 
 % \setlength{\belowcaptionskip}{-0.05cm} 
	\includegraphics[width=0.9\linewidth]{fig/interexample.pdf}
	\caption{ A simple example of our proposed virtual-instruction-based interrupt. }
	\label{fig:interexample}
\end{figure}

\begin{figure*}[t]
	\centering
   % \vspace{-0.1cm} 
	% \setlength{\abovecaptionskip}{0cm} 
   % \setlength{\belowcaptionskip}{-0.05cm} 
	\includegraphics[width=0.99\linewidth]{fig/barresult.pdf}
	\caption{Fig(a), comparison for interrupt CNN-based PR in DSLAM between different method. Fig(b), latency comparison between layer-by-layer interrupt method and our virtual-instruction (VI) method. }
	\label{fig:barresult}
   \end{figure*}
   
   \begin{figure}[t]
	\centering
   % \vspace{-0.1cm} 
	% \setlength{\abovecaptionskip}{0cm} 
   % \setlength{\belowcaptionskip}{-0.05cm} 
	\includegraphics[width=0.9\linewidth]{fig/env.pdf}
	\caption{Multi-robot exploration: environment and results. }
	\label{fig:env}
   \end{figure}


\Cref{fig:interexample}(a) is the instruction sequence from DDR with VI-ISA. The instructions are generated for the scheduling shown in {fig:singlesave}. The Vir\_SAVE instruction is responsible for backing up executing status, i.e., saving the output results of the first CALC\_F. The Vir\_LOAD\_D instruction restore the input featuremaps from DDR to on-chip memory. The Vir\_LOAD. The SAVE instruction in \Cref{fig:interexample}(a) saves all the output results for the two CALC\_F.
\Cref{fig:interexample}(b) is the original ISA instructions translated by the IAU without interrupt. The virtual instructions (Vir\_SAVE and Vir\_LOAD) are skipped and discarded by the IAU. Thus the accelerator receives the original ISA sequence without backup or restore instructions.
When an interrupt occurs at the first CalcBlob, \Cref{fig:interexample}(c) illustrates the backup/recovery instructions (Blue) and the modified SAVE instruction (Red). Because the output results of the first CALC\_F are stored to DDR with the fitst SAVE, which is translated from the Vir\_SAVE in fig(a), the last SAVE instruction is modified to only store the output results of the second CALC\_F.


